{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network with Keras ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will learn how to use Keras library to build convolutional neural network. We will also use again the MNIST dataset andcompare our results to using conventional neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use Keras library to build convolutional neural networks.\n",
    "2. Convolutional Neural Network with One Convolutional and Pooling Layers.\n",
    "3. Convolutional Neural Networks with two Convolutional and Pooling Layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the librarues and packages that we would need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup complete\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "print(\"setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with convolutional neural network in particular, we need additional packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Layen with One Set of convolutional and pooling layers ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# reshape to be [samples][pixels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalice the pixel values to be between 0 and 1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to convert the target variable into binary categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# and extract the number of categories\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define a function that create our model. Let's start with one set of convolutional and pooling layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_model():\n",
    "    #create a model \n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16, (5, 5), strides=(1, 1), activation='relu', input_shape=(28, 28, 1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation ='softmax'))\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tray calling the function to see how it performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "300/300 - 21s - loss: 1.8410 - accuracy: 0.8694 - val_loss: 0.2550 - val_accuracy: 0.9355\n",
      "Epoch 2/10\n",
      "300/300 - 15s - loss: 0.1736 - accuracy: 0.9544 - val_loss: 0.1631 - val_accuracy: 0.9582\n",
      "Epoch 3/10\n",
      "300/300 - 15s - loss: 0.0965 - accuracy: 0.9730 - val_loss: 0.1179 - val_accuracy: 0.9682\n",
      "Epoch 4/10\n",
      "300/300 - 15s - loss: 0.0615 - accuracy: 0.9815 - val_loss: 0.1179 - val_accuracy: 0.9699\n",
      "Epoch 5/10\n",
      "300/300 - 16s - loss: 0.0426 - accuracy: 0.9866 - val_loss: 0.1029 - val_accuracy: 0.9730\n",
      "Epoch 6/10\n",
      "300/300 - 15s - loss: 0.0315 - accuracy: 0.9899 - val_loss: 0.1053 - val_accuracy: 0.9737\n",
      "Epoch 7/10\n",
      "300/300 - 16s - loss: 0.0245 - accuracy: 0.9920 - val_loss: 0.0994 - val_accuracy: 0.9755\n",
      "Epoch 8/10\n",
      "300/300 - 15s - loss: 0.0184 - accuracy: 0.9940 - val_loss: 0.0829 - val_accuracy: 0.9786\n",
      "Epoch 9/10\n",
      "300/300 - 15s - loss: 0.0157 - accuracy: 0.9950 - val_loss: 0.0891 - val_accuracy: 0.9798\n",
      "Epoch 10/10\n",
      "300/300 - 16s - loss: 0.0199 - accuracy: 0.9937 - val_loss: 0.1006 - val_accuracy: 0.9787\n",
      "Accuracy: 0.9786999821662903 \n",
      " Error: 2.1300017833709717\n"
     ]
    }
   ],
   "source": [
    "# build model \n",
    "model = convolutional_model()\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: {} \\n Error: {}\". format(scores[1], 100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convolutional Layer with two sets of convolutional and pooling layers ##\n",
    "\n",
    "Let's redefine our convolutional model so that it has two convolutional and pooling layers instead of just one layer of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_model():\n",
    "    \n",
    "    # create a model\n",
    "    model =Sequential()\n",
    "    model.add(Conv2D(16, (5, 5), activation ='relu', input_shape=(28, 28, 1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(8, (2, 2), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    #compile model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "300/300 - 23s - loss: 2.9003 - accuracy: 0.7587 - val_loss: 0.3585 - val_accuracy: 0.9113\n",
      "Epoch 2/10\n",
      "300/300 - 18s - loss: 0.2801 - accuracy: 0.9268 - val_loss: 0.2089 - val_accuracy: 0.9447\n",
      "Epoch 3/10\n",
      "300/300 - 17s - loss: 0.1753 - accuracy: 0.9513 - val_loss: 0.1598 - val_accuracy: 0.9550\n",
      "Epoch 4/10\n",
      "300/300 - 21s - loss: 0.1302 - accuracy: 0.9632 - val_loss: 0.1371 - val_accuracy: 0.9607\n",
      "Epoch 5/10\n",
      "300/300 - 18s - loss: 0.1009 - accuracy: 0.9698 - val_loss: 0.1184 - val_accuracy: 0.9677\n",
      "Epoch 6/10\n",
      "300/300 - 22s - loss: 0.0843 - accuracy: 0.9750 - val_loss: 0.1087 - val_accuracy: 0.9694\n",
      "Epoch 7/10\n",
      "300/300 - 22s - loss: 0.0721 - accuracy: 0.9776 - val_loss: 0.1003 - val_accuracy: 0.9710\n",
      "Epoch 8/10\n",
      "300/300 - 24s - loss: 0.0641 - accuracy: 0.9800 - val_loss: 0.0929 - val_accuracy: 0.9752\n",
      "Epoch 9/10\n",
      "300/300 - 21s - loss: 0.0548 - accuracy: 0.9824 - val_loss: 0.0872 - val_accuracy: 0.9758\n",
      "Epoch 10/10\n",
      "300/300 - 21s - loss: 0.0474 - accuracy: 0.9853 - val_loss: 0.0879 - val_accuracy: 0.9774\n",
      "Accuracy: 0.977400004863739 \n",
      " Error: 2.2599995136260986\n"
     ]
    }
   ],
   "source": [
    "model = convolutional_model()\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
    "\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: {} \\n Error: {}\".format(scores[1], 100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
